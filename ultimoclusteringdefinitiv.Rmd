---
title: "Clustering-USA-Cityzents"
author: "Alejandro Braun"
date: "27/2/2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_section: yes
    theme: yeti
    highlight: tango
  pdf_document:
    toc: yes
---

Cargamos el Dataset.
```{r}
census<-read.csv("census2000_conLongitudLatitud.csv",sep=",",header = T,stringsAsFactors = T)
head(census)
str(census)
```

Convirtamos las variables en númericas.
```{r,message=FALSE,warning=FALSE}
library(dplyr)
census$MeanHHSz<-as.numeric(census$MeanHHSz)
census$RegPop<-as.numeric(gsub(",","",census$RegPop))
census$RegDens<-as.numeric(census$RegDens)
census$MedHHInc<- substring(census$MedHHInc,2)
census$MedHHInc<-as.numeric(gsub(",","",census$MedHHInc))
census$ID<-as.numeric(census$ID)
census<- census %>%
  filter(!is.na(ID))
head(census)
```

Analicemos los datos.
```{r,message=FALSE,warning=FALSE}
library(funModeling)
library(GGally)
profiling_num(census)
status(census)
plot_num(census)
describe(census)
```

Podemos ver algunos valores característicos de los datos así como su distribución.

Alta presencia de NA,s en REgDens, presencia de 0 en RegPop y MeanHHSD, de hecho, RegDens tiene el mismo 
número de NA,s que que ceros REgPop, puede ser que coincidan. Veamos los NA,s gráficamente.
```{r,message=FALSE,warning=FALSE}
library(VIM)
aggr(census, col=c('navyblue','yellow'),
     numbers=TRUE, sortVars=TRUE,
     labels=names(census), cex.axis=.7,
     gap=3, ylab=c("Missing data","Pattern"))
```

Veamos si los NA,s de RegDens coinciden con los ceros de RegPop y de MeanHHsz.
```{r,message=FALSE}
census %>%
  filter(is.na(RegDens))%>%
  group_by(RegDens)%>%
  summarise(RegPop=unique(RegPop),MeanHHSz=unique(MeanHHSz))
```

Todos los valores NA,s de RegDens coinciden con ceros en las otras dos variables, por tanto eliminamos las filas con NA,s.
```{r}
census<-na.omit(census)
```

Discretizamos las variables.
```{r,message=FALSE,warning=FALSE}
library(nima)
census.scale<-census
census.scale$discreteRegDens <- discrete_by_quantile(census.scale$RegDens)/4
census.scale$discreteRegPop <- discrete_by_quantile(census.scale$RegPop)/4
census.scale$discreteMedHHInc <- discrete_by_quantile(census.scale$MedHHInc)/4
census.scale$discreteMeanHHSz <- discrete_by_quantile(census.scale$MeanHHSz)/4
summary(census.scale)
colnames(census.scale)[1]<-"ID"
```

Comprobemos la correlación entre las nuevas variables.
```{r,message=FALSE,warning=FALSE}
matrizCorrelacion<-cor(census.scale[,8:11],method=c("spearman"))
library(corrplot)
corrplot(matrizCorrelacion,method="number",type="upper")
```

No observamos una correlación excesivamente alta.

Para hacer clustering con el método jerárquico, primero extraeremos una muestra del 30%.
```{r}
set.seed(2354)
indexes<-sample(1:nrow(census.scale),size=0.3*nrow(census.scale))
census.scale.muestra<-census.scale[indexes,]
dim(census.scale.muestra)
```

Hacemos la matriz de distancias y dibujamos un dendograma.
```{r,message=FALSE,warning=FALSE}
memory.limit(size=40000)
library(vegan)
library(cluster)
MatrizDistancias<-vegdist(census.scale.muestra[,8:11],method="euclidean")
clusterJerarquico<-hclust(MatrizDistancias,method="ward.D2")
plot(clusterJerarquico, labels = FALSE, main = "Dendrograma")
rect.hclust(clusterJerarquico, k=2, border="red") 
rect.hclust(clusterJerarquico, k=3, border="blue") 
rect.hclust(clusterJerarquico, k=4, border="green") 
rect.hclust(clusterJerarquico, k=5, border="yellow") 
rect.hclust(clusterJerarquico, k=6, border="purple") 
rect.hclust(clusterJerarquico, k=7, border="gray") 
rect.hclust(clusterJerarquico, k=8, border="black") 
```

A primera vista, a medida que va aumentando los clusters la ganancia disminuye, aún así, es difícil ver con exactitud cuantos clusters coger.

Veamos el método calinski y Harabasz. Este método escoge el número de clusters que maximizar la suma de las distancias entre clusters minimizando las distancias de los valores que pertenecen a cada cluster.
```{r}
calinsky <- cascadeKM(census.scale.muestra[,8:11], inf.gr = 2, sup.gr = 10, iter = 100, criterion = "calinski")
calinsky$results
```

Ambos métodos marcan que lo más óptimo es escoger dos clusters.

Veamos el gráfico de silueta, ponemos como máximo 20 clusters ya que no cogeremos más bajo ninguna circunstancia, los resultados deben ser interpretables.
```{r}
asw <- numeric(20)
for(k in 2:(20 - 1)){
  sil <- silhouette(cutree(clusterJerarquico, k = k), MatrizDistancias)
  asw[k] <- summary(sil)$avg.width}
k.best <- which.max(asw)

plot(1: 20, asw, type="h", 
     main = "Silhouette-optimal number of clusters", 
     xlab = "k (number of groups)", ylab = "Average silhouette width")
axis(1, k.best, paste("optimum", k.best, sep = "\n"), col = "red", font = 2,
     col.axis = "red")
points(k.best, max(asw), pch = 16, col = "red", cex = 1.5)
```

Nos indica que debríamos coger dos clusters según este método.

Apliquemos otro método.
```{r,message=FALSE,warning=FALSE}
library(GGally)
library(factoextra)
fviz_nbclust(census.scale.muestra, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

El método Silhouette mide la calidad de una agrupación y determina cómo se encuentra cada punto dentro de su agrupación.
Indica que los más óptimo es coger 3 clusters.


Se aplica el gráfico de Elbow al Dataset completo, aunque sea un método propio de k-means, nos puede ayudar a saber cuantos clusters coger, mi intención no es escoger sólo 2, ya que pretendo ver un mayor grado de diferenciación entre los datos. Seguimos el mismo criterio, no cogeremos más de 20 clusters.
```{r}
set.seed(13345)
n <- dim(census.scale)[1] 

p <- dim(census.scale[,8:11])[2]

SSW <- (n - 1) * sum(apply(census.scale[,8:11],2,var)) 

for (i in 2:20) SSW[i] <- 
  sum(kmeans(census.scale[,8:11],centers=i,nstart=3,iter.max=20)$withinss)

plot(1:20, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col="steelblue4")
```


Veamos el resultado de la gráfica numéricamente.
```{r}

mejora<-numeric()
anterior<-1
x<-1
for (i in SSW){
  if (anterior==1){mejora[x]<-1}
  else {mejora[x]<-(anterior-i)/anterior}
  anterior <- i
  x <- x+1
}
round(mejora,3)
```

Observando la evolución de la suma de cuadrado intragrupos observo que el porcentje de explicación de las variables empieza a aumentar en menos del 10% a partir del séptimo cluster. Tomaremos este valor, no quiero aumentar más el número de clusters para no perder demasiada interpretabilidad.

Calculemos los 7 centroides.
```{r}
asignacionJerarquica<-cbind(census.scale.muestra[,c(1:3,8:11)],
                            cutree(clusterJerarquico, k = 7))
colnames(asignacionJerarquica)[8]<-"cluster"
centroidesJerarquico<-asignacionJerarquica%>%
  group_by(cluster)%>%
  summarise(total=n(),discreteRegDens=mean(discreteRegDens),discreteRegPop=mean(discreteRegPop),
            discreteHHInc=mean(discreteMedHHInc),discreteMeanHHSz=mean(discreteMeanHHSz))
centroidesJerarquico
```

Observemos que porcentaje de nuestros datos representa cada cluster.
```{r}
porcentaje<-cbind(centroidesJerarquico$cluster,porcentaje=centroidesJerarquico$total/
                    sum(centroidesJerarquico$total))
porcentaje
```

Introdujamos los centroides en el método k-means para poder ver así los clusters definitivos.
```{r}
kmeans <- kmeans(census.scale[,8:11],centers=centroidesJerarquico[,3:6])
kmeans$centers
```

Estos son nuestros clusters definitivos, los introducimos en nuestro Dataset.
```{r}
census<-cbind(census,kmeans$cluster)
head(census)
colnames(census)[8]<-"cluster"
```

Hagamos los gráficos de radar para ver los valores de cada cluster.
```{r,message=FALSE,warning=FALSE}
library(fmsb)



centroidesOptimizacionParaRadar<-rbind(
  rep(1,4) , 
  rep(0,4) , 
  apply(kmeans$centers , 2, mean),
  kmeans$centers)

colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) )
colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) )


  tamanyo<-centroidesJerarquico[1,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,4),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
  

```

Los habitantes del primer grupo forman parte de un area con familias muy grandes, la densisdad está por debajo de la media, el número de habitantes es ligeramente menor a la media, pueden ser ciudades grandes con baja densidad, los ingresos están por debajo de la media, llamaremos a este cluster "nucleos familiares grandes en ciudades poco densas". 
```{r}
  tamanyo<-centroidesJerarquico[2,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,5),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```

Este cluster cuenta con una densidad ligeramente por debajo de la media y pocas personas por región, pueden ser areas rurales, el tamaño familiar es bajo y los ingresos también, lo llamaremos "areas rurales empobrecidas".Cabe destacar que este es el cluster con un mayor número de pertenecientes.
```{r}
  tamanyo<-centroidesJerarquico[3,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,6),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```

Este cluster cuenta con una densidad de población y número de habitantes muy alto, deben ser ciudades importantes, los ingresos están ligeramente por encima de la media mientras que el tamaño familiar es bajo, por lo tanto puden ser areas donde la gente se muda para trabajar, posiblemente muchas personas de este cluster sean jóvenes que se han independizado y que aún no han formado una familia, lo llamaremos "grandes ciudades juveniles".
```{r}
  tamanyo<-centroidesJerarquico[4,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,7),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```

El cuarto grupo pertenece a los habitantes propios de un aera con una gran densidad de población, sim embargo con un número de habitantes de  región menor que la media, deben ser ciudades pequeñas pobladas, sus ingresos y tamaño familiar están por debajo de la media, lo llamaremos "areas urbanas densas y pequeñas". Este el cluster con un menor número de pertenecientes.
```{r}
  tamanyo<-centroidesJerarquico[5,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,8),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```


El quinto cluster tiene todo por encima de la media, especialmente el nivel de ingresos y el tamaño familiar, dene ser ciudades con un areas grandes, con mucho trabajo y un PIB per capita muy alto, lo llamaremos "familias enriquecidas de grandes ciudades". Cabe destacar que este cluster también cuenta con un gran número de pertenecientes.
```{r}
  tamanyo<-centroidesJerarquico[6,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,9),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```

A este grupo pertenecen habitantes de areas poco pobladas y con una baja densidad de población, el tamaño familiar es también bastante bajo mientras el nivel de ingresos esta bastante por encima de la media, por eso creo que sona areas rurales. Lo llamaremos "areas rurales enriquecidas".
```{r}
  tamanyo<-centroidesJerarquico[7,2]
  
  radarchart( as.data.frame(centroidesOptimizacionParaRadar[c(1:3,10),])  , axistype=1 ,
              pcol=colors_border , pfcol=colors_in , plwd=1 , plty=1,
              cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,1,5), cglwd=0.8,
              vlcex=0.8,
              title=paste0("Tamaño:",tamanyo))
```

El último grupo se caracteriza por una densidad de población y número de habitantes ligeramente por debajo de la media, mientas que el nivel de ingresos y el tamaño de la unidad familiar es bastante alto, lo llamaremos "areas familiares enriquecidas".

Por último, vamos a mapear nuestrso resultados para tener una visión global de cómo se encuentra distribuida los habitantes de cada cluster.
```{r,message=FALSE,warning=FALSE}
library(leaflet)
pal<-colorFactor(palette="Set1",
                 domain=census$cluster)
popup<-paste0('<b>tamañofam:</b> ', as.character(census$MeanHHSz), '<br>',
              '<b>ingresos:</b>', as.character(census$MedHHInc), '<br>',
              '<b>densidad:</b>', as.character(census$RegDens), '<br>' ,
              '<b>población:</b>', as.character(census$RegPop))
leaflet(data=census)%>%
  setView(lng=mean(census$LocX), lat=mean(census$LocY),zoom=2)%>%
  addTiles(urlTemplate = 'http://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png')%>%
  addCircles(~LocX,~LocY,popup = popup,color=~pal(cluster))
  

```

